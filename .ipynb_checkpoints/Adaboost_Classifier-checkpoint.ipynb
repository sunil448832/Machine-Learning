{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sunil448832/Machine-Learning/blob/main/Adaboost_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y1pHrLpNnKNS"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HvTL8k9D9Km"
   },
   "source": [
    "# **Loading Breast Cancer Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvx3DVuYnqHJ",
    "outputId": "a6ca00db-9a1a-48a2-df61-31bda0ed7ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n",
      "(398,)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5N3BY7oELFm"
   },
   "source": [
    "# **Implementation of single decision stump**\n",
    "```\n",
    "This decision stump takes a single feature and corresponding label\n",
    "and finds best single feature classifier which gives minimum  error on the given training feature\n",
    "it returns the stump as [threshold,m or l,error]\n",
    "where m= it classifies 1 if feature value is more than threshold else 0.\n",
    "      l= it classifies 1 if feature value is less than threshold else 0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tsGEofBwh2se"
   },
   "outputs": [],
   "source": [
    "def decision_stump(feature,label):\n",
    "  temp=list(zip(feature,label))\n",
    "  temp=sorted(temp ,key=lambda x:x[0])\n",
    "  feature,label=zip(*temp)\n",
    "  thresholds=[(feature[i-1]+feature[i])/2 for i in range(1,len(feature))]\n",
    "\n",
    "  gini_index,type_list=[],[]\n",
    "  for thres in thresholds:\n",
    "    cor_l,incor_l,cor_r,incor_r=0,0,0,0\n",
    "    for idx ,(i,j) in enumerate(zip(feature,label)):\n",
    "        if i<=thres and j==1 or i>thres and j==0:cor_l+=1\n",
    "        if i>thres and j==1 or i<=thres and j==0:cor_r+=1\n",
    "            \n",
    "    incor_l=len(label)-cor_l\n",
    "    incor_r=len(label)-cor_r\n",
    "    \n",
    "    gini_l=1-(cor_l/(cor_l+incor_l))**2-(incor_l/(cor_l+incor_l))**2\n",
    "    gini_r=1-(cor_r/(cor_r+incor_r))**2-(incor_r/(cor_r+incor_r))**2\n",
    "    type='l' if gini_l<gini_l else 'm'\n",
    "    gini=gini_l*(cor_l+incor_l)/(cor_l+incor_l+cor_r+incor_r)+gini_r*(cor_r+incor_r)/(cor_l+incor_l+cor_r+incor_r)\n",
    "    gini_index.append(gini)\n",
    "    type_list.append(type)\n",
    "    \n",
    "  min_g=min(gini_index)\n",
    "  min_index=gini_index.index(min_g)\n",
    "  return thresholds[min_index],type_list[min_index],min_g\n",
    "\n",
    "# x=[[10,30,50,70,25,10,40,100,20,120],\n",
    "#    [.9,.8,.7,.5,.5,.66,.1,.2,.3,.1],\n",
    "#   [1000,500,400,100,200,300,700,100,800,600]]\n",
    "# x=np.array(x).T\n",
    "# y=[0, 0, 1, 1, 0, 0, 1, 1,1,1]\n",
    "# print(decision_stump(x[:,0],y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG4gJl30GigC"
   },
   "source": [
    "#**Best Stump selection based on Gini index from all Stumps**\n",
    "```\n",
    "We find all the stumps by passing individual features and corresponding label in decision_stump function one by one\n",
    "and select the best Feature and Stump which gives minimum Gini Index value.\n",
    "this functions returns [feature index,threshold,m or l,minimum error]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hy2_y88AFZ_P"
   },
   "outputs": [],
   "source": [
    "def DecisionTreeClassifier(features,label):\n",
    "    errors,stumps,type_list=[],[],[]\n",
    "    features=features.T\n",
    "    for f in features:\n",
    "        stump,type,err=decision_stump(f,label)\n",
    "        errors.append(err)\n",
    "        stumps.append(stump)\n",
    "        type_list.append(type)\n",
    "    min_val=min(errors)\n",
    "    min_index=errors.index(min_val)\n",
    "    return [min_index,stumps[min_index],type_list[min_index]],min_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx2HXB6iKpB9"
   },
   "source": [
    "# **Preparing new Dataset from Old Dataset**\n",
    "`Selection of new sample is done by random number. if generated random number is grearter than the  cummulative weights of the sample we add that sample to new Dataset.we have size of the random numbers equal to size of dataset.so sample having hiegher weights will se selected frequently.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-aqZmALPKl7r"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(features,label,w):\n",
    "    dataset,labels=[],[]\n",
    "    length=len(label)\n",
    "    c_w=np.zeros(length+1)\n",
    "    for i in range(0,length):\n",
    "        c_w[i+1]=c_w[i]+w[i]\n",
    "    index=np.random.uniform(0,1,length)\n",
    "    idxx=[]\n",
    "    for i in index:\n",
    "        for idx in range(length):\n",
    "            if c_w[idx]<=i<=c_w[idx+1]:\n",
    "                f=features[idx].tolist()\n",
    "                dataset.append(f)\n",
    "                labels.append(label[idx])\n",
    "                idxx.append(idx)\n",
    "                break\n",
    "    return np.array(dataset),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1TImc5vWJYUR"
   },
   "outputs": [],
   "source": [
    "def predict(features,learners):\n",
    "    if learners[-1]=='m':\n",
    "        return 1 if features[learners[0]]>learners[1] else 0\n",
    "  \n",
    "    if learners[-1]=='l':\n",
    "        return 1 if features[learners[0]]<=learners[1] else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdVDzLacPWKN"
   },
   "source": [
    "#**AdaBoost Algorithm Implementation**\n",
    "\n",
    "```\n",
    "In this algorthm we assign equal weights to each samples and normalise to make weight sum equal to one.\n",
    "we run this algorithm T time to find best T weak Learners and corresponding votes.\n",
    "In Algorithm we first find the best decision stump(ie. Weak Learner)\n",
    "and this Weak Learner predict the labels for all sample in training set and accumulate all the error made by the Weak Learner say Total_Error.\n",
    "Then we calculate Vote for the Weak Learner as\n",
    "  Vote=(1/2)log(1-Total_Error)/Total_Error.\n",
    "  weight[sample] =weight[sample]*exp(Vote) if classified incorrect\n",
    "                 =weight[sample]*exp(-Vote) if classified correct\n",
    "\n",
    "ie. correctly classified sample have lower weights\n",
    "and incorrectly classified sampel have higher weights.\n",
    "This function returns list of T Weak Learners and corresponding Vote.\n",
    "as [[Weak Learner1,Vote1],[[Weak Learner2,Vote2]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XwnOmNfMnYw"
   },
   "outputs": [],
   "source": [
    "thres1=lambda x,y:1 if x!=y else 0 \n",
    "thres2=lambda x,y:1 if x!=y else 0\n",
    "def ada_boost1(features,label,T):\n",
    "  classifier_final=[]\n",
    "  alpha,index=[],[]\n",
    "  for t in range(T):\n",
    "    w=np.ones(len(label))/len(label)\n",
    "    h_list,error=[],[]\n",
    "    h,err=DecisionTreeClassifier(features,label)\n",
    "    total_error=0\n",
    "    for idx,(i,j) in enumerate(zip(features,label)):\n",
    "        pred=predict(i,h)\n",
    "        total_error+=w[idx]*thres1(pred,j)\n",
    "    \n",
    "\n",
    "    total_error=total_error+.0000000001\n",
    "    votes=np.log((1-total_error)/(total_error))/2\n",
    "    for idx,(i,j) in enumerate(zip(features,label)):\n",
    "        pred=predict(i,h)\n",
    "        w[idx]=w[idx]*np.exp(thres2(pred,j)*votes)\n",
    "    w=w/w.sum()\n",
    "    features,label=prepare_dataset(features,label,w)\n",
    "    classifier_final.append([h,votes])\n",
    "  return classifier_final\n",
    "\n",
    "learners= ada_boost1(X_train,Y_train,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJrW0HpievJ9",
    "outputId": "56825722-dd11-45ec-c1c0-6d7652b15541"
   },
   "outputs": [],
   "source": [
    "first_weak_learner,first_vote=learners[0]\n",
    "print(first_weak_learner)\n",
    "print(first_vote)\n",
    "print('feature numbers:  {}\\threshold value of feature:  {}\\nlearner type:  {}'.format(*first_weak_learner))\n",
    "print('Vote corresponding to Learner:  {}'.format(first_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGXiQlttbtKu"
   },
   "source": [
    "# **AdaBoost Classifier**\n",
    "```\n",
    "This is the Strong Classifier made by weigted combination of Weak Learners.\n",
    "here wieghts are the votes correspinding to each Weak Learner.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUSb0nacbPhE"
   },
   "outputs": [],
   "source": [
    "def ada_boost_classifier(features,learners):\n",
    "  count_0,count_1=0,0\n",
    "  for (l,alpha) in learners:\n",
    "    pred=predict(features,l)\n",
    "    if pred==1:count_1+=alpha\n",
    "    if pred==0:count_0+=alpha\n",
    "\n",
    "  return 1 if count_1>count_0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFBHAndZca2M"
   },
   "source": [
    "# **Testing the Adaoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLGZw7fHXpjj",
    "outputId": "f23ccdcd-20ed-4da3-ed78-7d14a8a35355"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for features,label in zip(X_test,Y_test):\n",
    "  pred=ada_boost_classifier(features,learners)\n",
    "  if pred==label:count+=1\n",
    "print('accuracy on the test set is:  %.2f'%(count/len(Y_test)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDiGMEYmUscHAfr70LKxyh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Adaboost_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
